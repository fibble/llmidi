# llmidi

> AI-powered MIDI composition through natural language conversations

![CI](https://github.com/fibble/llmidi/actions/workflows/ci.yml/badge.svg)

âš ï¸ **WORK IN PROGRESS** - This is an early-stage project and highly experimental! âš ï¸

**llmidi** is a planned open-source Electron application that will transform musical ideas into MIDI sequences using conversational AI. The vision: describe what you want to hear in plain English, and watch as AI generates the perfect musical parts for your compositions.

## ğŸš§ Current Status

This project is in **very early development**. Currently exploring concepts and gathering community input. Most features described below are planned functionality, not yet implemented.

**What exists now:**
- Project structure and planning
- Technology research and architecture decisions
- Community building and requirements gathering

**Coming next:**
- Basic Electron app setup
- OpenRouter API integration
- First MIDI generation prototype

## ğŸ¯ Vision & Planned Features

### ğŸµ Natural Language Music Generation *(Planned)*
- **Conversational Interface**: Describe musical ideas in plain English ("create a funky bassline", "add a jazzy piano melody")
- **Iterative Refinement**: Build and refine musical parts through natural conversation ("make it more syncopated", "add some slides")
- **Genre-Aware**: AI understands musical styles, moods, and terminology

### ğŸ¹ Multi-Track Composition *(Planned)*
- **Independent Tracks**: Generate bass, melody, chords, drums, and custom instrument parts
- **Per-Track Context**: Each track maintains its own conversation history for focused development
- **Inter-Track Awareness**: Generate parts that complement existing tracks ("create a melody that works with this bassline")

### ğŸ‘ï¸ Visual MIDI Display *(Planned)*
- **Piano Roll Views**: Clear visual representation of generated MIDI notes for each track
- **Real-Time Playback**: Visual playhead shows current playback position
- **Multi-Track Overview**: See all generated parts simultaneously with individual track controls

### ğŸ”Š Flexible Audio Preview *(Planned)*
- **Built-In Instruments**: Quick auditioning with piano, electric piano, strings, bass, and percussion
- **Professional Integration**: Route tracks to your DAW via virtual MIDI ports for high-quality VST playback
- **Solo/Mute Controls**: Compare and arrange parts with standard mixer-style controls

### ğŸ›ï¸ Professional DAW Integration *(Planned)*
- **Virtual MIDI Routing**: Each track appears as a separate MIDI device in your DAW
- **Live Recording**: Generate and record MIDI directly into DAW tracks during sessions
- **Standard Workflow**: Works with any DAW that supports MIDI input (Ableton, Logic, Pro Tools, FL Studio, etc.)

### ğŸ“ Export & Sharing *(Planned)*
- **MIDI Export**: Save individual tracks as standard MIDI files
- **Cross-Platform**: Works on Windows, macOS, and Linux
- **Open Source**: Fully transparent and community-driven development

## ğŸš€ Getting Started

### Development Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/fibble/llmidi.git
   cd llmidi
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Run in development mode**
   ```bash
   npm run dev
   ```

4. **Build for production**
   ```bash
   # For your current platform
   npm run build
   npm run dist

   # For all platforms
   npm run build
   npm run pack
   ```

### First Run

1. Launch the app in development mode
2. Click the Settings button in the header
3. Add your OpenRouter API key
4. Create your first track and start generating music!

### Available Scripts

- `npm run dev` - Start the app in development mode with hot reload
- `npm run build` - Build the app for production
- `npm run lint` - Run ESLint to check code quality
- `npm run format` - Format code with Prettier
- `npm run clean` - Clean build directories

## ğŸ¯ Planned Use Cases

### Rapid Ideation
- Break through writer's block with AI-generated musical sketches
- Explore new genres and musical styles
- Generate backing tracks for improvisation practice

### Professional Workflow Enhancement
- Create foundation tracks for further development in your DAW
- Generate reference parts for band arrangements
- Quickly prototype musical ideas before full production

### Learning & Experimentation
- Study AI-generated musical patterns and structures
- Experiment with unfamiliar musical styles
- Learn through iterative conversation with AI

## ğŸ› ï¸ Planned Technology Stack

- **Frontend**: Electron with Vue
- **AI Integration**: OpenRouter API for LLM access
- **MIDI Processing**: Node.js MIDI libraries
- **Audio Engine**: Tone.js for built-in instrument playback
- **Visualization**: Custom piano roll components

## ğŸ¤ Contributing & Community

llmidi is being developed in the open and welcomes early contributors and feedback! 

**How to get involved:**
- â­ **Star the repo** to show interest and follow progress
- ğŸ’¬ **Join discussions** to share ideas and requirements
- ğŸ› **Report issues** or suggest features
- ğŸ› ï¸ **Contribute code** as development progresses

**Areas where help is needed:**
- AI music generation prompt engineering
- MIDI library evaluation and integration
- Piano roll visualization design
- DAW integration testing
- UX/UI design for musicians

## ğŸ“‹ Development Roadmap

- [ ] **Phase 1**: Basic Electron app setup and UI framework
- [ ] **Phase 2**: OpenRouter integration and basic MIDI generation
- [ ] **Phase 3**: Piano roll visualization and playback
- [ ] **Phase 4**: Multi-track support and conversation context
- [ ] **Phase 5**: DAW integration and virtual MIDI routing
- [ ] **Phase 6**: Polish, performance, and production release

*Roadmap will be updated as development progresses*

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¼ Project Vision

Music creation should be accessible to everyone. llmidi aims to bridge the gap between musical imagination and technical implementation, allowing anyone to transform creative ideas into professional MIDI sequences through simple conversation.

**Why this project?**
- Lower the barrier to music creation
- Enhance existing DAW workflows with AI assistance
- Explore the creative potential of conversational music generation
- Build an open-source alternative to closed AI music tools

---

[ğŸŒŸ Star this repo](../../stargazers) | [ğŸ› Report Issues](../../issues) | [ğŸ’¬ Join Discussions](../../discussions)
